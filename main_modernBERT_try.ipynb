{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Глобальные константы, прописываем в отдельные преременные для удобства\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "PATH_TO_RAW_DATA = os.path.join('data', 'raw', 'geo-reviews-dataset-2023.csv')\n",
    "NUM_LABLES = 6\n",
    "BATCH_SIZE = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объем сырого датасэта: 500000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>name_ru</th>\n",
       "      <th>rating</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Екатеринбург, ул. Московская / ул. Волгоградск...</td>\n",
       "      <td>Московский квартал</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Жилой комплекс</td>\n",
       "      <td>Московский квартал 2.\\nШумно : летом по ночам ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Московская область, Электросталь, проспект Лен...</td>\n",
       "      <td>Продукты Ермолино</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Магазин продуктов;Продукты глубокой заморозки;...</td>\n",
       "      <td>Замечательная сеть магазинов в общем, хороший ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Краснодар, Прикубанский внутригородской округ,...</td>\n",
       "      <td>LimeFit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Фитнес-клуб</td>\n",
       "      <td>Не знаю смутят ли кого-то данные правила, но я...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Санкт-Петербург, проспект Энгельса, 111, корп. 1</td>\n",
       "      <td>Snow-Express</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Пункт проката;Прокат велосипедов;Сапсёрфинг</td>\n",
       "      <td>Хорошие условия аренды. \\nДружелюбный персонал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Тверь, Волоколамский проспект, 39</td>\n",
       "      <td>Студия Beauty Brow</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Салон красоты;Визажисты, стилисты;Салон бровей...</td>\n",
       "      <td>Топ мастер Ангелина топ во всех смыслах ) Немн...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             address             name_ru  \\\n",
       "0  Екатеринбург, ул. Московская / ул. Волгоградск...  Московский квартал   \n",
       "1  Московская область, Электросталь, проспект Лен...   Продукты Ермолино   \n",
       "2  Краснодар, Прикубанский внутригородской округ,...             LimeFit   \n",
       "3   Санкт-Петербург, проспект Энгельса, 111, корп. 1        Snow-Express   \n",
       "4                  Тверь, Волоколамский проспект, 39  Студия Beauty Brow   \n",
       "\n",
       "   rating                                            rubrics  \\\n",
       "0     3.0                                     Жилой комплекс   \n",
       "1     5.0  Магазин продуктов;Продукты глубокой заморозки;...   \n",
       "2     1.0                                        Фитнес-клуб   \n",
       "3     4.0        Пункт проката;Прокат велосипедов;Сапсёрфинг   \n",
       "4     5.0  Салон красоты;Визажисты, стилисты;Салон бровей...   \n",
       "\n",
       "                                                text  \n",
       "0  Московский квартал 2.\\nШумно : летом по ночам ...  \n",
       "1  Замечательная сеть магазинов в общем, хороший ...  \n",
       "2  Не знаю смутят ли кого-то данные правила, но я...  \n",
       "3  Хорошие условия аренды. \\nДружелюбный персонал...  \n",
       "4  Топ мастер Ангелина топ во всех смыслах ) Немн...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем датасэт\n",
    "\n",
    "raw_data_set = pd.read_csv(PATH_TO_RAW_DATA)\n",
    "print(f'Объем сырого датасэта: {len(raw_data_set)}')\n",
    "raw_data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 5., 1., 4., 2., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lables = raw_data_set['rating'].unique() # возвращает объект типа np.ndarray\n",
    "lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Бьем на train и test, defaul shuffle = True\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(raw_data_set['text'], raw_data_set['rating'], train_size=0.3, test_size=0.1, random_state=RANDOM_SEED, stratify=raw_data_set['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Токенизируем тексты\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny')\n",
    "train_tokens = tokenizer(list(train_X), padding = True, truncation=True)\n",
    "test_tokens = tokenizer(list(test_X), padding = True, truncation=True)\n",
    "\n",
    "print(train_tokens.keys())\n",
    "print(train_tokens['input_ids'][0])\n",
    "print(tokenizer.decode(train_tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем оболочку для хранения и передачи в модель наших данных\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self,\n",
    "                 tokenized_text,\n",
    "                 attention_mask,\n",
    "                 lables):\n",
    "        self.tokenized_text = torch.tensor(tokenized_text)\n",
    "        self.attention_mask = torch.tensor(attention_mask)\n",
    "        self.lables = torch.tensor(lables.to_numpy())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lables)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_text[idx], self.attention_mask[idx], self.lables[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Выбираем доступное место для наших вычислений\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем два необходимых даталоадера\n",
    "\n",
    "train_dataset = CustomDataSet(train_tokens['input_ids'], train_tokens['attention_mask'], train_Y)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = CustomDataSet(test_tokens['input_ids'], test_tokens['attention_mask'], test_Y)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Загружаем нужную модель\n",
    "\n",
    "#model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=NUM_LABLES)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('answerdotai/ModernBERT-base', num_labels=NUM_LABLES)\n",
    "\n",
    "# model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\") # Pre-trained model\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5) # Optimization function\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(28996, 768, padding_idx=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Узнаем как выглядят токены:\n",
    "\n",
    "test_id = train_tokens['input_ids'][0][1]\n",
    "with torch.no_grad():\n",
    "    test_embedding = model.get_input_embeddings()\n",
    "\n",
    "test_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.7718e-02, -8.0116e-03, -7.8941e-03,  9.6660e-03, -1.0245e-01,\n",
       "         1.6066e-02,  2.1629e-02,  8.2826e-03,  6.0495e-03, -1.9429e-02,\n",
       "        -2.5278e-02, -8.5322e-02, -5.2908e-02, -2.8096e-02,  8.1911e-03,\n",
       "         2.0108e-02, -4.6732e-02, -6.1263e-02, -1.0914e-01, -2.9416e-02,\n",
       "        -3.9170e-02, -2.9461e-02,  3.4286e-02, -1.2243e-01, -4.7996e-02,\n",
       "        -7.2637e-03,  2.6350e-02, -2.3867e-02, -2.2668e-02, -7.4090e-04,\n",
       "        -7.7993e-02,  9.0500e-02,  1.4340e-02, -4.4981e-02, -5.3229e-02,\n",
       "        -1.6184e-02, -5.2363e-02, -4.3146e-02,  2.1146e-03, -2.4799e-02,\n",
       "         6.8474e-02, -3.5734e-02, -5.0536e-02, -3.5180e-02, -1.8313e-02,\n",
       "         4.4827e-02, -7.1537e-02, -3.1704e-02, -8.4373e-02, -3.4681e-02,\n",
       "        -4.0438e-02, -5.6349e-02, -5.8097e-02, -1.4534e-02, -6.6772e-02,\n",
       "         4.2977e-02, -1.5955e-02, -2.1121e-02, -7.8810e-02, -1.1700e-02,\n",
       "        -7.2549e-02, -2.7447e-02, -5.3368e-02, -6.0594e-03,  3.4049e-02,\n",
       "        -7.0013e-02, -7.1642e-04, -2.4598e-02,  9.5299e-03,  2.5774e-03,\n",
       "         4.9075e-02, -3.6767e-02,  5.6192e-03, -6.5581e-02, -2.0475e-02,\n",
       "        -8.1344e-04, -8.3113e-02,  1.9895e-02, -1.1160e-01, -3.1413e-02,\n",
       "         4.3622e-03, -7.2774e-02,  4.3081e-03, -1.2864e-02,  4.8634e-02,\n",
       "        -1.7965e-02, -2.6516e-02,  5.5783e-03, -7.2484e-02, -6.5003e-03,\n",
       "        -9.1670e-02,  2.5314e-02, -9.0613e-02, -7.5314e-02, -4.8168e-02,\n",
       "        -4.2643e-02, -1.6638e-02, -4.5662e-02, -6.6803e-02, -7.1246e-02,\n",
       "        -6.1006e-02, -1.8138e-02, -4.9317e-02, -6.7010e-02,  3.3947e-02,\n",
       "        -4.0979e-02, -5.7818e-02, -3.7062e-02, -3.5027e-02, -7.9054e-02,\n",
       "        -2.3929e-02, -5.8611e-02, -1.0873e-01,  9.8740e-02,  3.1582e-02,\n",
       "        -5.1763e-02, -2.9876e-02, -3.1398e-05, -2.4739e-02, -6.1841e-02,\n",
       "         2.5078e-03, -4.7495e-03, -9.8187e-03, -8.6652e-02,  7.5275e-03,\n",
       "        -1.2260e-02, -1.8864e-02, -4.2189e-04, -1.0661e-01, -3.0349e-02,\n",
       "        -2.4508e-02,  4.1620e-02,  4.8941e-02,  6.6947e-03, -2.7484e-02,\n",
       "        -4.2651e-03,  6.1097e-02,  2.0168e-02, -4.8453e-02, -4.0802e-02,\n",
       "         7.3063e-02, -1.2060e-01,  3.1637e-02, -6.1950e-02, -4.7823e-02,\n",
       "        -4.4061e-02, -6.9553e-02, -5.6160e-02, -6.3690e-02,  8.2698e-03,\n",
       "        -5.1805e-03,  5.1966e-03,  4.2249e-02, -2.1802e-03, -1.2118e-01,\n",
       "         7.7869e-03, -6.2357e-02, -6.4975e-02, -1.1888e-02, -6.0026e-02,\n",
       "         9.0937e-02, -3.1673e-02,  4.9626e-02,  4.0129e-02,  2.7903e-02,\n",
       "         4.0493e-03,  6.0984e-03, -1.1660e-01, -1.0180e-01, -4.3939e-02,\n",
       "        -2.4767e-02,  4.7586e-02, -4.0551e-02, -6.0861e-02, -1.8895e-02,\n",
       "         2.8999e-03, -4.8429e-03, -2.5885e-02, -5.0358e-02, -3.7658e-02,\n",
       "        -1.8395e-02, -1.2347e-02, -1.8712e-02, -5.6952e-03, -1.1439e-01,\n",
       "        -7.4508e-02, -2.2495e-02,  4.8004e-02, -9.8373e-02, -6.9401e-03,\n",
       "        -9.0851e-02,  3.4133e-02, -5.1124e-02, -4.3734e-02, -3.5763e-02,\n",
       "        -4.2004e-02, -2.4300e-03,  8.4970e-02, -2.6996e-02, -2.0918e-02,\n",
       "        -5.5863e-02, -7.0435e-02,  1.4955e-02, -2.0335e-02,  2.0737e-02,\n",
       "        -4.2981e-02, -6.2183e-04,  1.3503e-02,  3.4814e-04,  9.8072e-03,\n",
       "        -6.4707e-02, -3.4537e-02, -2.9339e-02, -1.6159e-02,  1.7292e-02,\n",
       "        -4.8799e-02, -3.8256e-02, -9.4184e-03, -2.3930e-02, -4.2189e-02,\n",
       "        -6.8056e-04, -5.5902e-02, -5.3644e-02, -1.7158e-02, -5.3415e-02,\n",
       "        -2.7371e-02, -1.0029e-02, -3.5532e-02, -2.6085e-02,  3.3914e-02,\n",
       "        -2.8904e-02, -4.8085e-02,  2.0069e-02, -4.3367e-02, -2.8290e-02,\n",
       "        -5.9904e-02,  4.6381e-02, -7.0011e-02, -3.5581e-02, -5.6910e-02,\n",
       "        -2.9717e-02, -7.3671e-02, -4.8677e-02, -4.3276e-02,  3.8689e-02,\n",
       "        -1.3085e-02,  2.4925e-03, -7.7252e-02, -5.7568e-02, -4.4687e-02,\n",
       "        -1.0885e-02, -4.8691e-02, -8.9533e-02,  6.9097e-03,  2.8038e-03,\n",
       "        -9.7857e-03, -1.1513e-01,  8.6017e-02, -9.2428e-02,  1.6377e-02,\n",
       "         7.0877e-02, -4.1411e-02, -5.0641e-03, -1.7886e-02, -1.0754e-01,\n",
       "        -3.5417e-02,  2.6156e-02, -2.9805e-02,  2.5054e-02, -4.4687e-02,\n",
       "        -3.2945e-02,  6.6930e-03, -1.0746e-02,  4.0155e-02, -2.9538e-02,\n",
       "         2.5341e-02, -7.8343e-02,  4.0769e-02, -1.0754e-02,  3.8085e-02,\n",
       "        -3.2414e-02, -1.5027e-02, -3.6507e-02,  2.4116e-02, -4.7668e-02,\n",
       "         2.8258e-02, -5.0551e-02,  9.0514e-03, -1.6372e-02, -1.8338e-02,\n",
       "         1.2988e-02, -2.7086e-02, -6.7875e-02, -2.7433e-02, -2.9568e-02,\n",
       "        -7.1976e-02, -2.1842e-03,  6.3613e-02, -1.7600e-02,  1.2534e-02,\n",
       "        -5.4890e-02,  4.1856e-04, -1.0972e-01, -2.1558e-02,  9.0694e-02,\n",
       "        -2.4237e-02, -4.4590e-02, -3.7291e-03, -2.1684e-02, -2.4185e-02,\n",
       "        -1.6842e-02, -5.3048e-02,  6.0764e-03,  3.9076e-02, -1.2895e-02,\n",
       "        -2.5246e-02, -8.6901e-02, -3.7041e-02, -1.0755e-02,  3.2187e-02,\n",
       "         4.5767e-02,  3.2859e-02, -3.3505e-02,  7.8849e-04,  3.7480e-03,\n",
       "        -1.9004e-02, -2.8649e-02,  7.8014e-04,  5.2832e-02, -9.5280e-02,\n",
       "         3.4960e-02, -8.3679e-02,  1.2632e-02, -3.1972e-02, -2.0711e-02,\n",
       "        -3.8293e-02, -2.2809e-02, -1.5064e-02, -7.9701e-02, -7.1104e-02,\n",
       "         2.6204e-02, -4.2774e-02, -8.0718e-02, -8.4522e-02, -7.2819e-02,\n",
       "         5.7201e-03, -1.0079e-01, -4.6067e-02,  1.7739e-02,  5.0085e-02,\n",
       "         1.2130e-02, -1.8782e-02, -1.0554e-01,  3.5144e-03, -2.0510e-02,\n",
       "        -9.8325e-02, -1.1464e-02, -2.1913e-02, -8.3199e-02, -4.0868e-02,\n",
       "        -4.8352e-02,  3.6169e-02, -6.0538e-02, -1.0652e-01,  7.0832e-03,\n",
       "        -2.3326e-02,  2.1919e-02, -1.0159e-02, -9.4595e-02, -2.9467e-03,\n",
       "        -8.2856e-02, -6.3566e-02,  2.4295e-02, -5.5092e-02, -1.1967e-01,\n",
       "        -9.7154e-02, -6.4389e-02, -2.1874e-02, -4.7072e-02,  4.7711e-02,\n",
       "        -2.3540e-03,  2.8251e-03,  2.3545e-02, -1.5286e-03, -5.0843e-02,\n",
       "         2.7687e-02, -3.1102e-02, -2.4335e-02, -5.3972e-02, -3.2220e-02,\n",
       "        -9.1776e-02,  3.0854e-02, -6.1046e-02, -1.0681e-01, -1.0117e-02,\n",
       "        -3.6086e-02, -7.3039e-02, -4.2905e-02, -8.1157e-02, -1.6276e-02,\n",
       "        -7.3934e-02, -3.9752e-02, -7.3346e-02, -2.3252e-02, -2.7286e-02,\n",
       "        -5.1880e-02, -2.0430e-02, -1.9323e-02, -6.1127e-02, -2.9225e-02,\n",
       "        -8.9203e-03, -3.5320e-02, -5.3197e-02, -2.9089e-02,  2.5644e-03,\n",
       "         5.6480e-02, -6.2936e-02, -9.0058e-02, -8.8778e-02, -2.7991e-02,\n",
       "         4.0395e-02, -1.9306e-02, -6.9425e-02,  4.9342e-02, -6.8821e-02,\n",
       "        -6.0944e-03,  5.1811e-02,  1.1822e-01, -1.7283e-02,  4.2348e-02,\n",
       "        -1.7305e-02,  2.5227e-02, -1.3811e-02, -1.1585e-01,  1.2126e-02,\n",
       "        -3.5208e-02, -7.0133e-02, -4.3175e-02, -6.6141e-02, -5.4679e-02,\n",
       "        -5.0833e-02, -3.5046e-02, -2.0758e-02,  1.2673e-02,  7.2538e-03,\n",
       "         1.1774e-02, -4.3751e-02,  1.1241e-02, -3.3287e-02, -4.6483e-02,\n",
       "        -3.7363e-02,  3.0061e-02, -7.4174e-02,  1.8760e-02, -3.6984e-02,\n",
       "         4.0792e-02, -4.3565e-02, -1.6165e-02, -1.8604e-02, -6.0715e-02,\n",
       "         1.2278e-03, -7.0024e-02,  2.6101e-03, -2.1600e-02,  5.4012e-03,\n",
       "        -8.6474e-02, -4.8053e-02, -4.1152e-02, -4.5346e-02, -7.8584e-02,\n",
       "        -5.8925e-02, -6.6620e-02, -2.9687e-02,  1.7588e-02, -2.1353e-02,\n",
       "        -1.1235e-02, -7.8276e-02,  6.4800e-02, -4.5138e-02,  1.4939e-02,\n",
       "        -1.6347e-02, -5.0336e-02, -2.4047e-02, -8.6185e-02, -6.9660e-02,\n",
       "        -4.6852e-03, -8.9618e-02, -2.7509e-02,  1.2799e-02, -9.2366e-02,\n",
       "        -8.1666e-02, -6.8461e-02, -1.7259e-02,  1.8647e-02,  2.6256e-03,\n",
       "        -7.3274e-02, -3.7703e-03, -4.6776e-02, -5.1367e-02,  1.0104e-02,\n",
       "         4.3829e-02, -4.7822e-02,  1.7882e-02, -1.8998e-02, -6.5899e-02,\n",
       "        -3.8274e-02, -3.3396e-02, -6.8828e-02, -2.7618e-02, -3.7298e-02,\n",
       "        -5.3639e-02, -4.0334e-02, -5.2340e-02,  5.7305e-02, -2.3710e-02,\n",
       "        -9.3989e-02, -2.3586e-03, -5.1465e-02,  6.4196e-04,  1.1871e-02,\n",
       "         6.0899e-03,  3.4391e-03, -9.6973e-02, -8.5718e-02, -4.1797e-02,\n",
       "        -1.5407e-02, -1.1530e-01, -5.6542e-02, -1.0205e-01, -5.2178e-03,\n",
       "        -5.7192e-02, -8.5606e-04, -8.7764e-02, -7.2509e-02, -2.1474e-02,\n",
       "        -7.4094e-02, -6.6470e-02, -5.8668e-02, -1.3659e-02, -5.9585e-03,\n",
       "        -7.2038e-03, -4.2329e-02, -2.1731e-02, -1.0998e-01, -1.0034e-01,\n",
       "        -7.2267e-02, -3.8139e-02,  3.1015e-02, -3.7238e-02, -5.6209e-02,\n",
       "        -3.7183e-02, -2.8788e-02, -7.3157e-02,  4.5254e-02, -2.7794e-02,\n",
       "         2.8157e-02, -6.5582e-02, -4.2023e-02,  2.6677e-02, -3.8170e-02,\n",
       "        -4.4874e-03, -2.8527e-02,  7.0328e-02, -7.0141e-02, -3.8275e-02,\n",
       "        -1.7884e-02, -5.1251e-02, -2.9484e-02,  3.2892e-03, -3.2344e-02,\n",
       "         2.0753e-02, -1.1507e-02, -3.4989e-02, -2.9970e-02, -1.2199e-01,\n",
       "        -2.7611e-02,  2.8713e-02,  1.6387e-02, -7.8859e-02, -1.3799e-02,\n",
       "        -5.1134e-02, -3.5730e-02,  4.8087e-02, -2.7905e-02,  1.2884e-02,\n",
       "         1.4198e-02, -7.7277e-02, -1.0933e-01, -5.4688e-02, -6.1039e-02,\n",
       "        -3.6610e-02,  5.8694e-02, -2.0862e-02, -4.7550e-02,  4.0652e-03,\n",
       "        -1.4336e-02,  2.8459e-02,  1.1586e-02, -6.0646e-02, -3.6406e-02,\n",
       "        -1.4547e-02,  1.9122e-02, -8.0013e-02,  3.9531e-02,  1.6492e-02,\n",
       "        -1.3708e-02, -8.0764e-02, -3.5156e-02, -7.7050e-03, -2.9543e-02,\n",
       "        -4.0615e-02, -1.2377e-01, -4.5054e-02, -1.1071e-01, -3.0648e-03,\n",
       "        -7.1391e-02, -4.6807e-03, -6.4825e-02, -7.9391e-02, -6.2803e-02,\n",
       "        -3.6396e-02, -4.0862e-02, -3.5653e-02, -8.5302e-02, -6.7933e-02,\n",
       "        -8.6047e-02,  3.6609e-03,  1.4058e-02, -7.1048e-04, -6.5069e-02,\n",
       "         2.4955e-02, -8.8266e-02,  1.8367e-02,  5.7981e-02, -6.4164e-03,\n",
       "        -2.1236e-03,  1.1773e-03, -3.2733e-02, -3.2121e-02,  3.2781e-03,\n",
       "        -4.6003e-02, -1.4223e-02, -4.9481e-02, -6.1890e-02, -1.8797e-03,\n",
       "        -3.4102e-02,  8.1580e-02, -1.0166e-02, -5.7117e-02, -3.3465e-03,\n",
       "        -3.1377e-02,  4.7761e-03,  5.5711e-02,  1.0449e-01, -6.8045e-02,\n",
       "        -4.0017e-02,  3.6974e-02, -5.4497e-03, -9.5217e-03,  3.0652e-03,\n",
       "        -1.6482e-02, -2.0125e-02, -1.3206e-02, -4.2144e-02, -2.2114e-02,\n",
       "         9.5959e-03, -3.2123e-02,  7.0776e-02, -2.1814e-02, -2.0398e-02,\n",
       "         1.5039e-02, -5.9420e-02, -5.0152e-02, -5.0887e-02, -1.9877e-02,\n",
       "        -7.7366e-03,  3.7259e-02, -1.0502e-01, -1.5582e-02, -3.9222e-02,\n",
       "        -9.0154e-02,  3.8477e-03, -4.0204e-02, -7.5123e-02, -5.3054e-02,\n",
       "        -3.7087e-02, -1.0538e-02, -1.0440e-01, -4.3286e-02, -1.3878e-02,\n",
       "        -1.3476e-03, -1.8070e-02, -4.6055e-02, -7.1968e-02, -1.2677e-01,\n",
       "        -1.1129e-01, -8.4391e-02, -1.3005e-02, -2.8578e-02, -5.8244e-02,\n",
       "        -2.6366e-02, -7.8921e-02,  5.8699e-02, -6.4324e-02, -1.8922e-02,\n",
       "        -6.5027e-02, -4.3913e-02, -3.9093e-02, -3.8257e-02, -2.2389e-02,\n",
       "        -4.1380e-02, -2.5780e-02, -3.7754e-02, -3.9934e-02,  1.2601e-02,\n",
       "        -5.0327e-02, -3.2893e-02, -2.0583e-02,  5.2326e-02, -1.3747e-02,\n",
       "        -1.0362e-01, -2.3516e-02,  1.2914e-02, -1.6626e-02, -1.0452e-02,\n",
       "         8.3437e-02, -7.2680e-02, -3.1156e-02, -4.7207e-02, -2.0523e-02,\n",
       "        -6.6132e-02,  7.9486e-02, -5.5916e-02, -3.2032e-02, -1.8736e-02,\n",
       "        -8.9617e-02,  2.8282e-02, -6.1996e-02, -2.1291e-02,  3.6995e-02,\n",
       "        -9.9702e-02, -2.1908e-02,  4.4438e-02, -4.9622e-02, -2.1194e-02,\n",
       "        -3.7069e-02, -8.4729e-02,  2.6386e-02, -5.8325e-03,  3.9321e-02,\n",
       "        -7.0260e-02, -6.1347e-02, -5.3100e-04, -5.7359e-03,  2.8705e-02,\n",
       "        -6.8201e-02, -7.0624e-03,  1.1180e-02, -6.5526e-02,  1.1302e-02,\n",
       "         1.0037e-02, -2.2469e-02,  8.3083e-03,  3.7140e-02, -2.1736e-02,\n",
       "        -2.4292e-02, -1.4701e-02,  3.4296e-02], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedding.weight[test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "model.to(device) # Transfer model to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/3750 [01:23<12:21:05, 11.88s/it]\n",
      "  0%|          | 0/3 [01:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Calculating the gradient for the loss function\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Optimizing the parameters of the bert model\u001b[39;00m\n\u001b[32m     33\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ЦИКЛ ОБУЧЕНИЯ\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_losses_per_epoch = []\n",
    "val_losses_per_epoch = []\n",
    "\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    # активируем обучение модели\n",
    "    model.train()\n",
    "    for batch in tqdm.tqdm(train_loader):\n",
    "\n",
    "        tokenized_text, attention_mask, lables = batch\n",
    "        lables = lables.type(torch.LongTensor)\n",
    "        tokenized_text, attention_mask, lables = tokenized_text.to(device), attention_mask.to(device), lables.to(device)\n",
    "        \n",
    "        # Сбрасываем градиенты с прошлых шагов\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(input_ids = tokenized_text, attention_mask = attention_mask)\n",
    "        \n",
    "        # Получаем логиты и передаем их в функцию потерь\n",
    "        pred = outputs.logits\n",
    "        loss = loss_fn(pred, lables)\n",
    "\n",
    "        # Обратное распростронение\n",
    "        loss.backward()\n",
    "        \n",
    "        # Шаг оптимизатора\n",
    "        optimizer.step()\n",
    "\n",
    "        # Сохраняем значение функции потерь для статистики \n",
    "        train_losses_per_epoch.append(loss.item())\n",
    "\n",
    "    train_losses.append(np.mean(train_losses_per_epoch))\n",
    "\n",
    "    #Замораживаем обучение\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            tokenized_text, attention_mask, lables = batch\n",
    "            lables = lables.type(torch.LongTensor)\n",
    "            tokenized_text, attention_mask, lables = tokenized_text.to(device), attention_mask.to(device), lables.to(device)\n",
    "            \n",
    "            # forward\n",
    "            outputs = model(input_ids = tokenized_text, attention_mask = attention_mask)\n",
    "            \n",
    "            # Получаем логиты и передаем их в функцию потерь\n",
    "            logits = outputs.logitsbels\n",
    "            loss = loss_fn(logits, lables)\n",
    "\n",
    "            # Сохраняем значение функции потерь для статистики\n",
    "            val_losses_per_epoch.append(loss.item())\n",
    "\n",
    "    val_losses.append(np.mean(val_losses_per_epoch))\n",
    "    val_losses_per_epoch = []\n",
    "    train_losses_per_epoch = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
